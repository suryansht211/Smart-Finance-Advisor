# -*- coding: utf-8 -*-
"""AI-Driven Real-Time Investment Risk Controlling and Management

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PIRn-D1UrsWzZbIqvLPNawi1wrNibcRY
"""

# Start ZooKeeper (required by Kafka)
bin/zookeeper-server-start.sh config/zookeeper.properties

# Start Kafka broker
bin/kafka-server-start.sh config/server.properties

import random
import time
from kafka import KafkaProducer
from kafka.errors import KafkaError

# Initialize Kafka producer
producer = KafkaProducer(bootstrap_servers='localhost:9092')

# Simulated data generation function
def generate_data():
    while True:
        try:
            # Generate random market data
            market_price = round(random.uniform(100, 500), 2)
            asset_performance = round(random.uniform(-5, 5), 2)
            economic_indicator = round(random.uniform(0, 100), 2)

            # Prepare data message
            data = f"{market_price},{asset_performance},{economic_indicator}"

            # Send data to Kafka topic 'investment_data'
            producer.send('investment_data', value=data.encode('utf-8'))
            print(f"Sent data: {data}")

            time.sleep(1)  # Simulate real-time data stream

        except KafkaError as ke:
            print(f"Error sending data to Kafka: {str(ke)}")
            time.sleep(5)  # Retry after 5 seconds

# Start data generation
if __name__ == "__main__":
    generate_data()

# Real-time data processing using Apache Spark (PySpark)
from pyspark.sql import SparkSession
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

# Initialize Spark session
spark = SparkSession.builder \
    .appName("RealTimeInvestmentAnalytics") \
    .getOrCreate()

# Initialize streaming context
ssc = StreamingContext(spark.sparkContext, 5)  # 5-second batch interval

# Connect to Kafka and subscribe to 'investment_data' topic
kafka_params = {"bootstrap.servers": "localhost:9092"}
topics = ["investment_data"]
kafka_stream = KafkaUtils.createDirectStream(ssc, topics, kafka_params)

# Process incoming Kafka stream
lines = kafka_stream.map(lambda x: x[1])
lines.pprint()  # Print raw data for demonstration

# Start streaming context
ssc.start()
ssc.awaitTermination()

# Placeholder for machine learning model development (e.g., using scikit-learn, TensorFlow, or PyTorch)
# Example: Risk assessment model with Random Forest Classifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd

# Sample data (replace with actual data processing pipeline)
data = {
    'MarketPrice': [300, 305, 310, 295, 290],
    'AssetPerformance': [0.5, 0.3, -0.2, -0.5, 0.7],
    'EconomicIndicator': [50, 55, 60, 45, 50],
    'RiskLevel': ['Low', 'Low', 'Medium', 'High', 'High']
}

df = pd.DataFrame(data)

# Features (X) and target variable (y)
X = df[['MarketPrice', 'AssetPerformance', 'EconomicIndicator']]
y = df['RiskLevel']

# Initialize Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Train the model
rf_classifier.fit(X, y)

# Example: Predict using the model
new_data = pd.DataFrame({
    'MarketPrice': [295, 300],
    'AssetPerformance': [-0.2, 0.1],
    'EconomicIndicator': [55, 58]
})

predictions = rf_classifier.predict(new_data)
print("Predicted Risk Levels:", predictions)

# Real-time dashboard using Plotly (simplified example)
import plotly.graph_objs as go
from plotly.subplots import make_subplots
import dash
import dash_core_components as dcc
import dash_html_components as html
from dash.dependencies import Input, Output

# Initialize Dash app
app = dash.Dash(__name__)

# Example: Update data and plot real-time charts
@app.callback(Output('live-update-graph', 'figure'),
              Input('interval-component', 'n_intervals'))
def update_graph(n):
    # Fetch and process real-time data here (replace with actual implementation)
    # Example: Generate random data for demonstration
    x_data = list(range(10))
    y_data = [random.uniform(100, 500) for _ in range(10)]

    # Create Plotly figure
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=x_data, y=y_data, mode='lines+markers', name='Market Price'))
    fig.update_layout(title='Real-Time Market Price',
                      xaxis_title='Time',
                      yaxis_title='Price')

    return fig

# Dash app layout
app.layout = html.Div([
    html.H1("Real-Time Investment Risk Dashboard"),
    dcc.Graph(id='live-update-graph'),
    dcc.Interval(
        id='interval-component',
        interval=1000,  # in milliseconds
        n_intervals=0
    )
])

if __name__ == '__main__':
    app.run_server(debug=True)