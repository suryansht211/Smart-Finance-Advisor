# -*- coding: utf-8 -*-
"""Risk_management

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ezO_7QSl7PPNVRllN8wiIWOtyP44Fbhi
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic data
def generate_synthetic_data(num_samples):
    data = {
        'transaction_amount': np.random.normal(1000, 500, num_samples),
        'account_balance': np.random.normal(10000, 5000, num_samples),
        'market_index': np.random.normal(2500, 300, num_samples),
        'customer_sentiment': np.random.choice([-1, 0, 1], num_samples),  # -1: negative, 0: neutral, 1: positive
        'operational_risk': np.random.choice([0, 1], num_samples, p=[0.9, 0.1])  # 0: low risk, 1: high risk
    }
    return pd.DataFrame(data)

# Generate a dataset with 1000 samples
synthetic_data = generate_synthetic_data(1000)
print(synthetic_data.head())

# Prepare data
X = synthetic_data[['transaction_amount', 'account_balance', 'market_index', 'customer_sentiment']]
y = synthetic_data['operational_risk']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a logistic regression model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Predict on the test set
y_pred = model.predict(X_test_scaled)

# Output the classification report
print(classification_report(y_test, y_pred))

# Generate risk report
def generate_risk_report(predictions, test_data):
    report = {
        'total_samples': len(predictions),
        'high_risk_count': np.sum(predictions),
        'low_risk_count': len(predictions) - np.sum(predictions),
        'high_risk_percentage': np.mean(predictions) * 100
    }
    return report

# Generate and print risk report
risk_report = generate_risk_report(y_pred, X_test)
print(risk_report)

import numpy as np
import pandas as pd
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns

# Download historical data for a financial instrument
def get_stock_data(ticker, start_date, end_date):
    stock_data = yf.download(ticker, start=start_date, end=end_date)
    stock_data['returns'] = stock_data['Adj Close'].pct_change()
    return stock_data

# Calculate Value at Risk (VaR) using the historical method
def calculate_var_historical(returns, confidence_level=0.95):
    sorted_returns = np.sort(returns.dropna())
    index = int((1 - confidence_level) * len(sorted_returns))
    var = sorted_returns[index]
    return var

# Visualize the returns distribution and VaR
def plot_var(returns, var):
    plt.figure(figsize=(10, 6))
    sns.histplot(returns, bins=50, kde=True, color='blue')
    plt.axvline(x=var, color='red', linestyle='--')
    plt.title(f'Value at Risk (VaR) at {confidence_level*100}% confidence level')
    plt.xlabel('Returns')
    plt.ylabel('Frequency')
    plt.show()

# User inputs
ticker = input("Enter the stock ticker symbol (e.g., 'AAPL' for Apple): ")
start_date = input("Enter the start date (YYYY-MM-DD): ")
end_date = input("Enter the end date (YYYY-MM-DD): ")
confidence_level = float(input("Enter the confidence level (e.g., 0.95 for 95%): "))

# Get stock data
stock_data = get_stock_data(ticker, start_date, end_date)

# Calculate VaR
var = calculate_var_historical(stock_data['returns'], confidence_level)

# Output VaR
print(f'Value at Risk (VaR) at {confidence_level*100}% confidence level: {var*100:.2f}%')

# Plot returns distribution and VaR
plot_var(stock_data['returns'], var)

"""Code for Financial Risk Modeling"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic data for different types of risks
def generate_synthetic_data(num_samples=1000):
    data = {
        'market_risk': np.random.normal(0, 1, num_samples),
        'credit_risk': np.random.choice([0, 1], num_samples, p=[0.9, 0.1]),
        'liquidity_risk': np.random.normal(0, 1, num_samples),
        'operational_risk': np.random.choice([0, 1], num_samples, p=[0.8, 0.2]),
        'interest_rate_risk': np.random.normal(0, 1, num_samples),
        'total_assets': np.random.normal(1e6, 1e5, num_samples),
        'total_liabilities': np.random.normal(5e5, 5e4, num_samples)
    }
    df = pd.DataFrame(data)
    df['dtr'] = df['total_liabilities'] / df['total_assets']
    df['market_risk_event'] = np.where(df['market_risk'] > 1, 1, 0)
    df['liquidity_risk_event'] = np.where(df['liquidity_risk'] > 1, 1, 0)
    df['interest_rate_risk_event'] = np.where(df['interest_rate_risk'] > 1, 1, 0)
    return df

# Generate synthetic dataset
synthetic_data = generate_synthetic_data()
print(synthetic_data.head())

# Prepare data for each risk type
risk_factors = ['market_risk', 'liquidity_risk', 'interest_rate_risk', 'dtr']

X_market = synthetic_data[risk_factors]
y_market = synthetic_data['market_risk_event']

X_credit = synthetic_data[risk_factors]
y_credit = synthetic_data['credit_risk']

X_liquidity = synthetic_data[risk_factors]
y_liquidity = synthetic_data['liquidity_risk_event']

X_operational = synthetic_data[risk_factors]
y_operational = synthetic_data['operational_risk']

X_interest_rate = synthetic_data[risk_factors]
y_interest_rate = synthetic_data['interest_rate_risk_event']

# Split data into training and testing sets
def split_and_scale_data(X, y):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    return X_train_scaled, X_test_scaled, y_train, y_test

# Model and evaluate risks
def model_and_evaluate_risk(X, y, risk_type):
    X_train, X_test, y_train, y_test = split_and_scale_data(X, y)
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"Classification Report for {risk_type}:")
    print(classification_report(y_test, y_pred))

# Market Risk
model_and_evaluate_risk(X_market, y_market, "Market Risk")

# Credit Risk
model_and_evaluate_risk(X_credit, y_credit, "Credit Risk")

# Liquidity Risk
model_and_evaluate_risk(X_liquidity, y_liquidity, "Liquidity Risk")

# Operational Risk
model_and_evaluate_risk(X_operational, y_operational, "Operational Risk")

# Interest Rate Risk
model_and_evaluate_risk(X_interest_rate, y_interest_rate, "Interest Rate Risk")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

# Set random seed for reproducibility
np.random.seed(42)

# Generate synthetic data for different types of risks
def generate_synthetic_data(num_samples):
    data = {
        'market_risk': np.random.choice([0, 1], num_samples, p=[0.7, 0.3]),  # 0: low risk, 1: high risk
        'credit_risk': np.random.choice([0, 1], num_samples, p=[0.8, 0.2]),
        'liquidity_risk': np.random.choice([0, 1], num_samples, p=[0.9, 0.1]),
        'operational_risk': np.random.choice([0, 1], num_samples, p=[0.85, 0.15]),
        'interest_rate_risk': np.random.choice([0, 1], num_samples, p=[0.75, 0.25]),
        'transaction_amount': np.random.normal(1000, 500, num_samples),
        'account_balance': np.random.normal(10000, 5000, num_samples),
        'market_index': np.random.normal(2500, 300, num_samples),
        'customer_sentiment': np.random.choice([-1, 0, 1], num_samples)  # -1: negative, 0: neutral, 1: positive
    }
    return pd.DataFrame(data)

# Generate a dataset with 1000 samples
num_samples = 1000
synthetic_data = generate_synthetic_data(num_samples)

# Function to model and evaluate risk
def model_risk(data, risk_type):
    X = data[['transaction_amount', 'account_balance', 'market_index', 'customer_sentiment']]
    y = data[risk_type]

    # Split data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Standardize the data
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train a Random Forest classifier
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train_scaled, y_train)

    # Predict on the test set
    y_pred = model.predict(X_test_scaled)

    # Output the classification report
    print(f"Classification Report for {risk_type.capitalize()}:\n")
    print(classification_report(y_test, y_pred))

    # Plot feature importance
    plt.figure(figsize=(10, 6))
    sns.barplot(x=model.feature_importances_, y=X.columns)
    plt.title(f'Feature Importance for {risk_type.capitalize()}')
    plt.show()

    # Plot the distribution of the predictions
    plt.figure(figsize=(10, 6))
    sns.histplot(y_pred, kde=False, bins=2)
    plt.title(f'Prediction Distribution for {risk_type.capitalize()}')
    plt.xlabel('Risk Level')
    plt.ylabel('Frequency')
    plt.show()

# Model and evaluate different types of risks
for risk in ['market_risk', 'credit_risk', 'liquidity_risk', 'operational_risk', 'interest_rate_risk']:
    model_risk(synthetic_data, risk)